{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6505d9d2",
   "metadata": {},
   "source": [
    "## *Importing The Libraries*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef6604af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os      # Import OS module for file handling\n",
    "import fitz    # PyMuPDF: A library for extracting text from PDFs\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import random\n",
    "import chromadb\n",
    "import ollama  # Import Ollama for chat model interaction\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602e840e",
   "metadata": {},
   "source": [
    "## PymyPDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffbb8e1",
   "metadata": {},
   "source": [
    "### *Text Extraction Function*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4aebeef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_text_from_pdfs(project_dataset):\n",
    "    \"\"\"Extracts text from all PDFs in a given folder.\"\"\"\n",
    "    all_text_chunks = []  # List to store extracted text chunks\n",
    "\n",
    "    for pdf_file in os.listdir(project_dataset):  # Loop through all files in the folder\n",
    "        if pdf_file.endswith(\".pdf\"):  # Process only PDF files\n",
    "            pdf_path = os.path.join(project_dataset, pdf_file)  # Get full file path\n",
    "            doc = fitz.open(pdf_path)  # Open the PDF file\n",
    "            \n",
    "            for page_num, page in enumerate(doc):  # Loop through each page\n",
    "                text = page.get_text(\"text\")  # Extract text from the page\n",
    "                if text.strip():  # Ignore empty pages\n",
    "                    all_text_chunks.append({  # Store extracted text with metadata\n",
    "                        \"content\": text,  \n",
    "                        \"metadata\": {\"source\": pdf_file, \"page\": page_num + 1}  \n",
    "                    })\n",
    "    \n",
    "    return all_text_chunks  # Return extracted text and metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10960d25",
   "metadata": {},
   "source": [
    "### *Calling The Function*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e30e1cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MuPDF error: syntax error: cannot find ExtGState resource 'GS0'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call the function and store the result\n",
    "project_dataset = r\"C:\\Users\\ronit\\project_dataset\"  # Replace with the actual folder path\n",
    "all_text_chunks = extract_text_from_pdfs(project_dataset)  # Call the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0041fcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "    doc = fitz.open(file_path)\n",
    "    extracted = []\n",
    "\n",
    "    for i, page in enumerate(doc):\n",
    "        try:\n",
    "            text = page.get_text()\n",
    "            extracted.append((i + 1, text))\n",
    "        except Exception as e:\n",
    "            print(f\"Error on page {i + 1}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return extracted\n",
    "\n",
    "    print(extracted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ebad57",
   "metadata": {},
   "source": [
    "### *Checking the Funtion*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b904b534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOVERNMENT OF INDIA \n",
      "MINISTRY OF RAILWAYS \n",
      "(RAILWAY BOARD) \n",
      "The General Managers(Comml), \n",
      "All Indian Railways.  \n",
      "Sub:- Auctioning of two-wheelers. \n",
      "         Chief Claims Officer, Southern Railway has brought to the notice of Ministry of Railways that at the \n",
      "time of booking of two-wheeler consignments no documentary evidence/proof is insisted upon in the \n",
      "forwarding note to establish genuineness of the vehicle. As and when unclaimed/undelivered two-wheelers \n",
      "put to public auction these either fetch very low bid or no bid as the intending bidders are not ready to buy \n",
      "them as it is difficult to get fresh registration for want of documents.  \n",
      "         Ministry of Railways have examined the matter and it has been decided that following procedure \n",
      "should be adopted by the railways for booking of two-wheelers:-  \n",
      "1. The booking clerk should ensure that the consignor fills his complete home address, telephone \n",
      "number etc. in the Forwarding Note.  \n",
      "2. Party should be asked to furnish a copy of R.C. at the time of booking of Two-wheelers.  \n",
      "3. Proper Labeling of the vehicle should be ensured so that it does not get unconnected.  \n",
      "4. If due to some reason, the two-wheeler gets unconnected the particulars of the same should be \n",
      "posted to the web. Before setting a claim for non-delivery of two-wheeler the setting railway must \n",
      "check up from the web.  \n",
      "5. After giving due publicity in the local newspapers the two-wheelers should put to public auction \n",
      "duly observing all commercial formalities and fixing up reserve price.  \n",
      "6. After the auction, a certificate to successful bidder should be issued to apply for fresh registration \n",
      "from RTA authorities.  \n",
      "         Board desire that the above instructions should be brought to the notice of all the concerned staff for \n",
      "compliance. Please acknowledge receipt of the letter.  \n",
      "Sd/-\n",
      "(Rajni Hasija)\n",
      "Director Traffic Commercial (Claims)\n",
      "Railway Board\n",
      "No.2004/TCIII/31.\n",
      "New Delhi, dated 16/03/2006.\n",
      "No.2004/TCIII/31.\n",
      "New Delhi, dated 16/03/2006.\n",
      "\n",
      "{'source': 'auction_two_wheeler.pdf', 'page': 1}\n"
     ]
    }
   ],
   "source": [
    "# all_text_chunks is defined, and we can loop through it\n",
    "target_pdf = \"auction_two_wheeler.pdf\"  # Change this to the specific file we want to check\n",
    "\n",
    "for chunk in all_text_chunks:\n",
    "    if chunk[\"metadata\"][\"page\"] == 1  and chunk[\"metadata\"][\"source\"] == target_pdf:\n",
    "        print(chunk[\"content\"])\n",
    "        print(chunk[\"metadata\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9b4cfd",
   "metadata": {},
   "source": [
    "## RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58259fd7",
   "metadata": {},
   "source": [
    "### *Chunking*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55ad8e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf(pdf_path):\n",
    "    \"\"\"Extracts text from a PDF and splits it into smaller chunks with metadata.\"\"\"\n",
    "    pdf_texts = extract_text_from_pdfs(pdf_path)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    \n",
    "    all_chunks = []\n",
    "    for text_entry in pdf_texts:\n",
    "        chunks = text_splitter.split_text(text_entry[\"content\"])\n",
    "        for chunk in chunks:\n",
    "            all_chunks.append({\"content\": chunk, \"metadata\": text_entry[\"metadata\"]})\n",
    "\n",
    "    return all_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed6944e",
   "metadata": {},
   "source": [
    "### *Checking The Function*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9112b9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MuPDF error: syntax error: cannot find ExtGState resource 'GS0'\n",
      "\n",
      "{'content': 'General services and also S.&T. Department, since\\nall the S. & T. and Electrical lines are cabled on\\naccount of Electrical Induction.\\nIn all A. C. and D. C. traction areas, cable\\nmarkers showing location of cables are provided by\\nthe Traction Department. In addition, the cables are\\nprotected by tiles and bricks, and during excavation\\nif workmen come across such tiles or bricks in an\\narranged manner, they should at once report the\\nmatter to the higher officials. Any further excavation', 'metadata': {'source': 'IRPWMUPTOACS148.pdf', 'page': 137}}\n"
     ]
    }
   ],
   "source": [
    "processed_chunks = process_pdf(project_dataset)\n",
    "print(random.choice(processed_chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d91c04a",
   "metadata": {},
   "source": [
    "\n",
    "## Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccf0bad",
   "metadata": {},
   "source": [
    "### *Sentence Transformer for Embeddings*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "baaa435e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "print(\"✅ Model loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2da50f4",
   "metadata": {},
   "source": [
    "### *Testing The Model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffd9b780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sample embedding generated. Length: 384\n"
     ]
    }
   ],
   "source": [
    "# Test the model with a sample sentence\n",
    "sample = \"Indian Railways is one of the largest rail networks in the world.\"\n",
    "embedding = embedding_model.encode(sample)\n",
    "print(\"✅ Sample embedding generated. Length:\", len(embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a962c28e",
   "metadata": {},
   "source": [
    "### *Database Deletion(if only a data base already exists)(only when running the whole code)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "910d9167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🗑️ Old ChromaDB removed.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "if os.path.exists(\"./chroma_db\"):\n",
    "    shutil.rmtree(\"./chroma_db\")\n",
    "    print(\"🗑️ Old ChromaDB removed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a7f485",
   "metadata": {},
   "source": [
    "### *Creating Vector Database Using ChromaDB*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed67a46",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Batch size 10317 exceeds maximum batch size 5461",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[59]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     30\u001b[39m     collection.add(\n\u001b[32m     31\u001b[39m         ids=ids,\n\u001b[32m     32\u001b[39m         documents=contents,\n\u001b[32m     33\u001b[39m         embeddings=converted_embeddings,\n\u001b[32m     34\u001b[39m         metadatas=metadatas\n\u001b[32m     35\u001b[39m     )\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m collection\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m collection = \u001b[43mstore_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_chunks\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[59]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mstore_embeddings\u001b[39m\u001b[34m(chunks)\u001b[39m\n\u001b[32m     27\u001b[39m     converted_embeddings.append(e.tolist())\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Add all at once to ChromaDB\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[43mcollection\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconverted_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m collection\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\RAG-llm\\.venv\\Lib\\site-packages\\chromadb\\api\\models\\Collection.py:91\u001b[39m, in \u001b[36mCollection.add\u001b[39m\u001b[34m(self, ids, embeddings, metadatas, documents, images, uris)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Add embeddings to the data store.\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m     63\u001b[39m \u001b[33;03m    ids: The ids of the embeddings you wish to add\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     79\u001b[39m \n\u001b[32m     80\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     82\u001b[39m add_request = \u001b[38;5;28mself\u001b[39m._validate_and_prepare_add_request(\n\u001b[32m     83\u001b[39m     ids=ids,\n\u001b[32m     84\u001b[39m     embeddings=embeddings,\n\u001b[32m   (...)\u001b[39m\u001b[32m     88\u001b[39m     uris=uris,\n\u001b[32m     89\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_add\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollection_id\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mids\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43membeddings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadatas\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdocuments\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m    \u001b[49m\u001b[43muris\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muris\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\RAG-llm\\.venv\\Lib\\site-packages\\chromadb\\telemetry\\opentelemetry\\__init__.py:150\u001b[39m, in \u001b[36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mglobal\u001b[39;00m tracer, granularity\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trace_granularity < granularity:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracer:\n\u001b[32m    152\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m f(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\RAG-llm\\.venv\\Lib\\site-packages\\chromadb\\api\\segment.py:103\u001b[39m, in \u001b[36mrate_limit.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: Any, **kwargs: Any) -> Any:\n\u001b[32m    102\u001b[39m     \u001b[38;5;28mself\u001b[39m = args[\u001b[32m0\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_rate_limit_enforcer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrate_limit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\RAG-llm\\.venv\\Lib\\site-packages\\chromadb\\rate_limit\\simple_rate_limit\\__init__.py:24\u001b[39m, in \u001b[36mSimpleRateLimitEnforcer.rate_limit.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: Any, **kwargs: Any) -> Any:\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\RAG-llm\\.venv\\Lib\\site-packages\\chromadb\\api\\segment.py:426\u001b[39m, in \u001b[36mSegmentAPI._add\u001b[39m\u001b[34m(self, ids, collection_id, embeddings, metadatas, documents, uris, tenant, database)\u001b[39m\n\u001b[32m    424\u001b[39m coll = \u001b[38;5;28mself\u001b[39m._get_collection(collection_id)\n\u001b[32m    425\u001b[39m \u001b[38;5;28mself\u001b[39m._manager.hint_use_collection(collection_id, t.Operation.ADD)\n\u001b[32m--> \u001b[39m\u001b[32m426\u001b[39m \u001b[43mvalidate_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    427\u001b[39m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muris\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_batch_size\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_max_batch_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    429\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    430\u001b[39m records_to_submit = \u001b[38;5;28mlist\u001b[39m(\n\u001b[32m    431\u001b[39m     _records(\n\u001b[32m    432\u001b[39m         t.Operation.ADD,\n\u001b[32m   (...)\u001b[39m\u001b[32m    438\u001b[39m     )\n\u001b[32m    439\u001b[39m )\n\u001b[32m    440\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_embedding_record_set(coll, records_to_submit)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\RAG-llm\\.venv\\Lib\\site-packages\\chromadb\\api\\types.py:833\u001b[39m, in \u001b[36mvalidate_batch\u001b[39m\u001b[34m(batch, limits)\u001b[39m\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvalidate_batch\u001b[39m(\n\u001b[32m    823\u001b[39m     batch: Tuple[\n\u001b[32m    824\u001b[39m         IDs,\n\u001b[32m   (...)\u001b[39m\u001b[32m    830\u001b[39m     limits: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[32m    831\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    832\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(batch[\u001b[32m0\u001b[39m]) > limits[\u001b[33m\"\u001b[39m\u001b[33mmax_batch_size\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    834\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBatch size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(batch[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m exceeds maximum batch size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlimits[\u001b[33m'\u001b[39m\u001b[33mmax_batch_size\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    835\u001b[39m         )\n",
      "\u001b[31mValueError\u001b[39m: Batch size 10317 exceeds maximum batch size 5461"
     ]
    }
   ],
   "source": [
    "def store_embeddings(chunks):\n",
    "    \"\"\"Encodes text chunks and stores them in ChromaDB with metadata.\"\"\"\n",
    "    chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "    collection = chroma_client.get_or_create_collection(name=\"railway_docs\")\n",
    "\n",
    "    # Log 1: Extract contents using explicit loop\n",
    "    contents = []\n",
    "    for c in chunks:\n",
    "        contents.append(c[\"content\"])\n",
    "\n",
    "    # ✅ Encode contents directly (no instruction needed)\n",
    "    embeddings = embedding_model.encode(contents)\n",
    "\n",
    "    # Log 2: Extract ids using explicit loop\n",
    "    ids = []\n",
    "    for i in range(len(chunks)):\n",
    "        ids.append(str(i))\n",
    "\n",
    "    # Log 3: Extract metadatas using explicit loop\n",
    "    metadatas = []\n",
    "    for c in chunks:\n",
    "        metadatas.append(c[\"metadata\"])\n",
    "\n",
    "    # Log 4: Convert embeddings to lists using explicit loop\n",
    "    converted_embeddings = []\n",
    "    for e in embeddings:\n",
    "        converted_embeddings.append(e.tolist())\n",
    "\n",
    "    # Add all at once to ChromaDB\n",
    "    collection.add(\n",
    "        ids=ids,\n",
    "        documents=contents,\n",
    "        embeddings=converted_embeddings,\n",
    "        metadatas=metadatas\n",
    "    )\n",
    "\n",
    "    return collection\n",
    "\n",
    "collection = store_embeddings(processed_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4640bd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_embeddings(chunks):\n",
    "    \"\"\"Encodes text chunks and stores them in ChromaDB with metadata in batches.\"\"\"\n",
    "    chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "    collection = chroma_client.get_or_create_collection(name=\"railway_docs\")\n",
    "\n",
    "    contents = []\n",
    "    for c in chunks:\n",
    "        contents.append(c[\"content\"])\n",
    "\n",
    "    embeddings = embedding_model.encode(contents)\n",
    "\n",
    "    ids = []\n",
    "    for i in range(len(chunks)):\n",
    "        ids.append(str(i))\n",
    "\n",
    "    metadatas = []\n",
    "    for c in chunks:\n",
    "        metadatas.append(c[\"metadata\"])\n",
    "\n",
    "    converted_embeddings = []\n",
    "    for e in embeddings:\n",
    "        converted_embeddings.append(e.tolist())\n",
    "\n",
    "    # ✅ Split into safe batches\n",
    "    BATCH_SIZE = 5000\n",
    "    total = len(ids)\n",
    "    \n",
    "    for i in range(0, total, BATCH_SIZE):\n",
    "        print(f\"Adding batch {i} to {min(i + BATCH_SIZE, total)}...\")\n",
    "        collection.add(\n",
    "            ids=ids[i:i + BATCH_SIZE],\n",
    "            documents=contents[i:i + BATCH_SIZE],\n",
    "            embeddings=converted_embeddings[i:i + BATCH_SIZE],\n",
    "            metadatas=metadatas[i:i + BATCH_SIZE]\n",
    "        )\n",
    "\n",
    "    return collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96fb3b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding batch 0 to 5000...\n",
      "Adding batch 5000 to 10000...\n",
      "Adding batch 10000 to 15000...\n",
      "Adding batch 15000 to 15128...\n"
     ]
    }
   ],
   "source": [
    "collection = store_embeddings(processed_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed8820a",
   "metadata": {},
   "source": [
    "## Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e330caa",
   "metadata": {},
   "source": [
    "### *Retrieve with Confidence Filter*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2daf4eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_chunks(query, collection, threshold=0.6):\n",
    "    \"\"\"Retrieves top chunks with confidence check.\"\"\"\n",
    "    query_emb = embedding_model.encode(query, convert_to_tensor=True)\n",
    "    results = collection.query(query_embeddings=[query_emb.tolist()], n_results=5, include=[\"distances\", \"metadatas\", \"documents\"])\n",
    "\n",
    "    top_distances = results.get(\"distances\", [[]])[0]\n",
    "    top_docs = results.get(\"documents\", [[]])[0]\n",
    "    top_meta = results.get(\"metadatas\", [[]])[0]\n",
    "\n",
    "    retrieved = []\n",
    "    for doc, meta, dist in zip(top_docs, top_meta, top_distances):\n",
    "        similarity = 1 - dist  # cosine distance to similarity\n",
    "        if similarity >= threshold:\n",
    "            retrieved.append({\"content\": doc, \"metadata\": meta})\n",
    "\n",
    "    return retrieved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9f6a8f",
   "metadata": {},
   "source": [
    "### *Answering With Ollama*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11a471e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_query(query, collection):\n",
    "    \"\"\"Generates response using retrieved context and Ollama.\"\"\"\n",
    "    LLM = \"llama3.1\"\n",
    "    chunks = retrieve_chunks(query, collection)\n",
    "\n",
    "    if not chunks:\n",
    "        return \"❌ No relevant info found for your query.\", []\n",
    "\n",
    "    context = \"\\n\\n\".join([c[\"content\"] for c in chunks])\n",
    "    metadata = [c[\"metadata\"] for c in chunks]\n",
    "    \n",
    "    prompt = f\"Use the following context to answer:\\n\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "\n",
    "    response = ollama.chat(model=LLM, messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "    return response[\"message\"][\"content\"], metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8982223",
   "metadata": {},
   "source": [
    "## Querying"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2fa427",
   "metadata": {},
   "source": [
    "### *Query and Retrieve*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e42752d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 3: Query and Retrieve ===\n",
    "query = \"Auctioning of two-wheelers?\"\n",
    "retrieved_chunks = retrieve_chunks(query, collection, threshold=0.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eda279e",
   "metadata": {},
   "source": [
    "### *Previewing Retrieved Chunks*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "997590e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Retrieved Chunks:\n",
      "\n",
      "[1]\n",
      "📄 Content Preview: put to public auction these either fetch very low bid or no bid as the intending bidders are not ready to buy  them as it is difficult to get fresh registration for want of documents.            Ministry of Railways have examined the matter and it has been decided that following procedure  should be\n",
      "📎 Metadata: {'page': 1, 'source': 'auction_two_wheeler.pdf'}\n"
     ]
    }
   ],
   "source": [
    "# === Step 4:  ===\n",
    "print(\"\\n🔍 Retrieved Chunks:\")\n",
    "for i, chunk in enumerate(retrieved_chunks):\n",
    "    print(f\"\\n[{i+1}]\")\n",
    "    print(\"📄 Content Preview:\", chunk[\"content\"][:300].strip().replace(\"\\n\", \" \"))\n",
    "    print(\"📎 Metadata:\", chunk[\"metadata\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f849f6",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "657f47e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧠 Answer:\n",
      " According to the context, it is mentioned that \"put to public auction these either fetch very low bid or no bid as the intending bidders are not ready to buy them...\" which implies that the intention is to put the two-wheelers up for public auction.\n",
      "\n",
      "✅ Relevant Sources:\n",
      "📎 Source: auction_two_wheeler.pdf, Page: 1\n"
     ]
    }
   ],
   "source": [
    "# === Step 5: Build Prompt ===\n",
    "def build_prompt(context, question):\n",
    "    return f\"\"\"\n",
    "You are a helpful assistant. Use the following extracted context to answer the question. \n",
    "If the answer is not found in the context, say \"Answer not found in the provided documents.\" \n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "context = \"\\n\\n\".join([chunk[\"content\"] for chunk in retrieved_chunks])\n",
    "prompt = build_prompt(context, query)\n",
    "\n",
    "# === Step 6: Get LLM Response ===\n",
    "response = ollama.chat(model=\"llama3.1\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "print(\"\\n🧠 Answer:\\n\", response[\"message\"][\"content\"])\n",
    "\n",
    "# === Step 7: Show Metadata Summary ===\n",
    "if retrieved_chunks:\n",
    "    metadata_info = [\n",
    "        f\"📎 Source: {chunk['metadata'].get('source', 'Unknown')}, Page: {chunk['metadata'].get('page', 'Unknown')}\"\n",
    "        for chunk in retrieved_chunks\n",
    "    ]\n",
    "    metadata_str = \"\\n\".join(metadata_info)\n",
    "    print(\"\\n✅ Relevant Sources:\\n\" + metadata_str)\n",
    "else:\n",
    "    print(\"\\n❌ No relevant info found for your query.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
